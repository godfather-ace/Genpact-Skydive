{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pinecone-client llama-index-vector-stores-pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    VectorStoreIndex, \n",
    "    StorageContext,\n",
    "    SimpleDirectoryReader, \n",
    ")\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# initializing Groq and Pinecone APIs\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "PINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]\n",
    "pc = Pinecone(api_key = PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM and Embedding Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = Groq(model = \"llama3-8b-8192\", api_key = GROQ_API_KEY)\n",
    "embed_model = HuggingFaceEmbedding(model_name = \"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pinecone Vector DB Index Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting index_name\n",
    "index_name = \"llama3-groq-pinecone\"\n",
    "\n",
    "# checking existing indexes\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# creating an index if it doesn't exists\n",
    "if index_name not in existing_indexes: \n",
    "    pc.create_index(\n",
    "        name = index_name, \n",
    "        dimension = 768, \n",
    "        metric = \"cosine\", \n",
    "        spec = ServerlessSpec(\n",
    "                cloud = \"aws\", \n",
    "                region = \"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# connecting to the created index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "\n",
    "# describing index statistics\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"sample_data_files\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upserting data in Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac6898d0ad84252b0713283786a881c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vector_store = PineconeVectorStore(pinecone_index = index)\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context = storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing a Query through LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the Dothraki horde do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dothraki horde broke the horizon and headed towards the city, 10,000 screamers strong. They then poured through the gaps in the Unsullied phalanxes, but their passage did not cause a single Unsullied to flinch. After that, they presented their customary show of strength before the city walls, a cyclone of horse and steel and screaming warriors, swirling before the immobile Unsullied. And then, without warning or apparent signal, they stopped in front of the Unsullied and fell silent, in ready position, eerily motionless, staring up at the nervous defenders of the city.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
